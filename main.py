# -*- coding: utf-8 -*-
"""minorfinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gfbDcFl3BnTnpj2jlaQOUlSLCFsNXMwB
"""

import pandas as pd
x=pd.read_csv("/content/drive/MyDrive/finaldataset.csv")
x.tail(10)

from sklearn.preprocessing import MinMaxScaler
scaling=MinMaxScaler()
scaling.fit_transform(x[["num"]])

x.isnull().sum()

#filling missing data using kNN

from sklearn.impute import KNNImputer

imputer = KNNImputer(n_neighbors=6)
#imputed = imputer.fit_transform(x[['RestBloodPressure','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']])
imputed = imputer.fit_transform(x)
#df_imputed = pd.DataFrame(imputed, columns=x.columns)
imputed

#imputed.isNaN().sum()
x_imputed = pd.DataFrame(imputed, columns=x.columns)
x_imputed

x_imputed.tail(10)

!pip install missingpy

#filling missing values using random forest
import sklearn.neighbors._base 
import sys
sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base
from missingpy import MissForest
imputer = MissForest()
xrf_imputed = imputer.fit_transform(x)
xrf_imputed

xrff_imputed = pd.DataFrame(xrf_imputed, columns=x.columns)
xrff_imputed

xrff_imputed.tail(10)

import sklearn
from sklearn.experimental import enable_iterative_imputer
from sklearn.linear_model import LinearRegression

from sklearn.impute import IterativeImputer
lr = LinearRegression()
imp = IterativeImputer(estimator=lr, verbose=2, max_iter=30, tol=1e-10, imputation_order='roman')

x_mice=imp.fit_transform(x)
x_mice

x_miice = pd.DataFrame(x_mice,columns=x.columns)
x_miice

x_miice.tail(10)

#filling missing data using mean value
import numpy as np
x["thal"]=x["thal"].replace(np.NaN,x["thal"].mean())
x["thal"][:20]

x["ca"]=x["ca"].replace(np.NaN,x["ca"].mean())
x["ca"][:20]

x["slope"]=x["slope"].replace(np.NaN,x["slope"].mean())
x["slope"][:20]

x["oldpeak"]=x["oldpeak"].replace(np.NaN,x["oldpeak"].mean())
x["oldpeak"][:20]

x["fbs"]=x["fbs"].replace(np.NaN,x["fbs"].mean())
x["fbs"][:20]

x["RestBloodPressure"]=x["RestBloodPressure"].replace(np.NaN,x["RestBloodPressure"].mean())
x["RestBloodPressure"][:20]

x["restecg"]=x["restecg"].replace(np.NaN,x["restecg"].mean())
x["restecg"][:20]

x["thalach"]=x["thalach"].replace(np.NaN,x["thalach"].mean())
x["thalach"][:20]

x["exang"]=x["exang"].replace(np.NaN,x["exang"].mean())
x["exang"][:20]

x["Chol"]=x["Chol"].replace(np.NaN,x["Chol"].mean())
x["Chol"][:20]

x.tail(10)

!pip install -q hvplot



from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

def print_score(clf, X_train, y_train, X_test, y_test, train=True):
    if train:
        pred = clf.predict(X_train)
        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))
        print("Train Result:\n================================================")
        print(f"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_train, pred)}\n")
        
    elif train==False:
        pred = clf.predict(X_test)
        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))
        print("Test Result:\n================================================")        
        print(f"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_test, pred)}\n")

#through mean
from sklearn.model_selection import train_test_split

X = x.drop('num', axis=1)
y = x.num

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#through mice
from sklearn.model_selection import train_test_split

X = x_miice.drop('num', axis=1)
y = x_miice.num

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""xgboost"""

from xgboost import XGBClassifier

#xgb_clf = XGBClassifier(use_label_encoder=False)
xgb_clf = XGBClassifier(learning_rate=0.1,n_estimators=100,loss="deviance")
model_XGBclassifier=xgb_clf.fit(X_train, y_train)
pred_xg = model_XGBclassifier.predict(X_test)
acc_xg = accuracy_score(y_test, pred_xg)  # evaluating accuracy score
print('accuracy score of XGB Classifier is:', acc_xg * 100)

print_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)
print_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)

test_score = accuracy_score(y_test, xgb_clf.predict(X_test)) * 100
train_score = accuracy_score(y_train, xgb_clf.predict(X_train)) * 100

results_df = pd.DataFrame(data=[["XGBoost Classifier", train_score, test_score]], 
                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])
#results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""SVM"""

from sklearn.svm import NuSVC

#svm_clf = SVC(kernel='rbf', gamma=0.1, C=1)

svm_clf = NuSVC(break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
      max_iter=-1, nu=0.28, probability=True, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
#svm_clf.fit(X_train, y_train)
model_SVMclassifier=svm_clf.fit(X_train, y_train)
pred_svc = model_SVMclassifier.predict(X_test)
acc_svc = accuracy_score(y_test, pred_svc)  # evaluating accuracy score
print('accuracy score of SVM Classifier is:', acc_svc * 100)

print_score(svm_clf, X_train, y_train, X_test, y_test, train=True)
print_score(svm_clf, X_train, y_train, X_test, y_test, train=False)

test_score = accuracy_score(y_test, svm_clf.predict(X_test)) * 100
train_score = accuracy_score(y_train, svm_clf.predict(X_train)) * 100

results_df_2 = pd.DataFrame(data=[["SVC", train_score, test_score]], 
                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""Adaboost"""

from sklearn.ensemble import AdaBoostClassifier
ad_clf=AdaBoostClassifier(learning_rate=1,n_estimators=50)
#ad_clf.fit(X_train, y_train)
model_adaboostclassifier=ad_clf.fit(X_train, y_train)
pred_ad = model_adaboostclassifier.predict(X_test)
acc_ad = accuracy_score(y_test, pred_ad)  # evaluating accuracy score
print('accuracy score of AdaBoost Classifier is:', acc_ad * 100)
print_score(ad_clf, X_train, y_train, X_test, y_test, train=True)
print_score(ad_clf, X_train, y_train, X_test, y_test, train=False)

test_score = accuracy_score(y_test, ad_clf.predict(X_test)) * 100
train_score = accuracy_score(y_train, ad_clf.predict(X_train)) * 100

results_df_2 = pd.DataFrame(data=[["AdaBoostClassifier", train_score, test_score]], 
                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""Gradient boosting classifier"""

from sklearn.ensemble import GradientBoostingClassifier
gb_clf=GradientBoostingClassifier(n_estimators=3,learning_rate=1)
#gb_clf.fit(X_train, y_train)
model_GradientBoostingclassifier=gb_clf.fit(X_train, y_train)
pred_gd = model_GradientBoostingclassifier.predict(X_test)
acc_gd = accuracy_score(y_test, pred_gd)  # evaluating accuracy score
print('accuracy score of Gradient boosting Classifier is:', acc_gd * 100)
print_score(gb_clf, X_train, y_train, X_test, y_test, train=True)
print_score(gb_clf, X_train, y_train, X_test, y_test, train=False)

test_score = accuracy_score(y_test, gb_clf.predict(X_test)) * 100
train_score = accuracy_score(y_train, gb_clf.predict(X_train)) * 100

results_df_2 = pd.DataFrame(data=[["GradientBoostingClassifier", train_score, test_score]], 
                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""Extra Trees Classifier"""

from sklearn.ensemble import ExtraTreesClassifier
#ext_clf=ExtraTreesClassifier()
ext_clf=ExtraTreesClassifier(n_estimators=75)
model_ExtraTreesclassifier=ext_clf.fit(X_train, y_train)
pred_et = model_ExtraTreesclassifier.predict(X_test)
acc_et = accuracy_score(y_test, pred_et)  # evaluating accuracy score
print('accuracy score of Extra Trees Classifier is:', acc_et * 100)
ext_clf.fit(X_train, y_train)
print_score(ext_clf, X_train, y_train, X_test, y_test, train=True)
print_score(ext_clf, X_train, y_train, X_test, y_test, train=False)

test_score = accuracy_score(y_test, ext_clf.predict(X_test)) * 100
train_score = accuracy_score(y_train, ext_clf.predict(X_train)) * 100

results_df_2 = pd.DataFrame(data=[["ExtraTreesClassifier", train_score, test_score]], 
                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""LGBM Classfier"""

#from sklearn.ensemble import LightGBMClassifier
#from lightgbm import LGBMRegressor
from lightgbm import LGBMClassifier
lgb=LGBMClassifier(learning_rate=0.009,n_estimators=1000,objective="binary")
#lgb.fit(X_train, y_train)
model_LGBMclassifier=lgb.fit(X_train, y_train)
pred_lgb = model_LGBMclassifier.predict(X_test)
acc_lgb = accuracy_score(y_test, pred_lgb)  # evaluating accuracy score
print('accuracy score of LGBM Classifier is:', acc_lgb * 100)
print_score(lgb, X_train, y_train, X_test, y_test, train=True)
print_score(lgb, X_train, y_train, X_test, y_test, train=False)

test_score = accuracy_score(y_test, lgb.predict(X_test)) * 100
train_score = accuracy_score(y_train, lgb.predict(X_train)) * 100

results_df_2 = pd.DataFrame(data=[["LGBMClassifier", train_score, test_score]], 
                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""SGDC Classifier"""

from sklearn.linear_model import SGDClassifier
sgd=SGDClassifier(loss="log",max_iter=1000,learning_rate="optimal")
#sgd.fit(X_train, y_train)
model_SGDCclassifier=sgd.fit(X_train, y_train)
pred_sgdc = model_SGDCclassifier.predict(X_test)
acc_sgdc = accuracy_score(y_test, pred_sgdc)  # evaluating accuracy score
print('accuracy score of  SGDC Classifier is:', acc_sgdc * 100)
print_score(sgd, X_train, y_train, X_test, y_test, train=True)
print_score(sgd, X_train, y_train, X_test, y_test, train=False)

test_score = accuracy_score(y_test, sgd.predict(X_test)) * 100
train_score = accuracy_score(y_train, sgd.predict(X_train)) * 100

results_df_2 = pd.DataFrame(data=[["SGD Classifier", train_score, test_score]], 
                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""meta classifier"""

!pip install mlxtend

!pip install pandas

!pip install -U scikit-learn

!pip install six

import six
import sys
sys.modules['sklearn.externals.six'] = six

import pandas as pd
import matplotlib.pyplot as plt
from mlxtend.plotting import plot_confusion_matrix
from mlxtend.classifier import StackingClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
#from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from mlxtend.classifier import StackingCVClassifier

scv = NuSVC(probability=True)  # defining meta-classifier
sclf = StackingClassifier(classifiers =[xgb_clf,ad_clf,gb_clf,ext_clf,lgb,sgd,svm_clf],meta_classifier = scv, use_probas = False, use_features_in_secondary = False)
#sclf = StackingClassifier(classifiers=[xgb_clf,svm_clf,ad_clf,gb_clf,ext_clf,lgb,sgd], 
                          #use_probas=True,
                          #meta_classifier=lr)
#sclf = StackingCVClassifier(classifiers = [xgb_clf,ad_clf,gb_clf,ext_clf,lgb,sgd,svm_clf],shuffle = False,use_probas = True,cv = 5,meta_classifier = scv)

model_stack = sclf.fit(X_train, y_train)  
pred_stack = model_stack.predict(X_test)

acc_stack = accuracy_score(y_test, pred_stack)  # evaluating accuracy
print('accuracy score of Stacked model:', acc_stack * 100)

from sklearn import model_selection

# Commented out IPython magic to ensure Python compatibility.
print('7-fold cross validation:\n')

for clf, label in zip([xgb_clf,svm_clf,ad_clf,gb_clf,ext_clf,lgb,sgd,sclf], 
                      ['xgboost', 
                       'svm', 
                       'adboost',
                       'Gradient',
                       'ExtraTrees',
                       'lightgbm',
                       'SGDC',
                       'StackingClassifier']):
  
    scores = model_selection.cross_val_score(clf, X, y, 
                                              cv=7, scoring='f1_macro')
    print("F1 Scores: %0.2f (+/- %0.2f) [%s]" 
#           % (scores.mean(), scores.std(), label))

r_probs = [0 for _ in range(len(y_test))]
xgb_probs = xgb_clf.predict_proba(X_test)
svm_probs = svm_clf.predict_proba(X_test)
ad_probs=ad_clf.predict_proba(X_test)
gb_probs=gb_clf.predict_proba(X_test)
ext_probs=ext_clf.predict_proba(X_test)
lgd_probs=lgb.predict_proba(X_test)
sgd_probs=sgd.predict_proba(X_test)
sclf_probs=sclf.predict_proba(X_test)
#Probabilities for the positive outcome is kept.

xgb_probs =xgb_probs[:, 1]
svm_probs = svm_probs[:, 1]
ad_probs =ad_probs[:, 1]
gb_probs = gb_probs[:, 1]
ext_probs =ext_probs[:, 1]
lgd_probs = lgd_probs[:, 1]
sgd_probs =sgd_probs[:, 1]
sclf_probs = sclf_probs[:, 1]

from sklearn.metrics import roc_curve, roc_auc_score

r_auc = roc_auc_score(y_test, r_probs)
xgb_auc = roc_auc_score(y_test, xgb_probs)
svm_auc = roc_auc_score(y_test, svm_probs)
ad_auc = roc_auc_score(y_test, ad_probs)
gb_auc= roc_auc_score(y_test, gb_probs)
ext_auc = roc_auc_score(y_test, ext_probs)
lgd_auc = roc_auc_score(y_test, lgd_probs)
sgd_auc = roc_auc_score(y_test, sgd_probs)
sclf_auc = roc_auc_score(y_test, sclf_probs)

#ad_auc = roc_auc_score(y_test, ad_probs)

print('Random (chance) Prediction: AUROC = %.3f' % (r_auc))
print('xgboost: AUROC = %.3f' % (xgb_auc))
print('nusvm: AUROC = %.3f' % (svm_auc))
print('adaboost: AUROC = %.3f' % (ad_auc))
print('gradient boost: AUROC = %.3f' % (gb_auc))
print('extra trees: AUROC = %.3f' % (ext_auc))
print('light gbm: AUROC = %.3f' % (lgd_auc))
print('sgd: AUROC = %.3f' % (sgd_auc))
print('sclf: AUROC = %.3f' % (sclf_auc))

r_fpr, r_tpr, _ = roc_curve(y_test, r_probs)
xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_probs)
nusvm_fpr, nusvm_tpr, _ = roc_curve(y_test, svm_probs)
ad_fpr, ad_tpr, _ = roc_curve(y_test, ad_probs)
gb_fpr, gb_tpr, _ = roc_curve(y_test, gb_probs)
extratrees_fpr, extratrees_tpr, _ = roc_curve(y_test, ext_probs)
lgbm_fpr, lgbm_tpr, _ = roc_curve(y_test, lgd_probs)
sgd_fpr, sgd_tpr, _ = roc_curve(y_test, sgd_probs)
sclf_fpr, sclf_tpr, _ = roc_curve(y_test, sclf_probs)

import matplotlib.pyplot as plt

plt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %0.3f)' % r_auc)
plt.plot(xgb_fpr, xgb_tpr, marker='.', label='xgb (AUROC = %0.3f)' % xgb_auc)
plt.plot(nusvm_fpr, nusvm_tpr, marker='.', label='nusvm (AUROC = %0.3f)' % svm_auc)
plt.plot(ad_fpr, ad_tpr, marker='.', label='ad (AUROC = %0.3f)' % ad_auc)
plt.plot(gb_fpr, gb_tpr, marker='.', label='gb (AUROC = %0.3f)' % gb_auc)
plt.plot(extratrees_fpr, extratrees_tpr, marker='.', label='extratrees (AUROC = %0.3f)' % ext_auc)
plt.plot(lgbm_fpr, lgbm_tpr, marker='.', label='lgbm (AUROC = %0.3f)' % lgd_auc)
plt.plot(sgd_fpr, sgd_tpr, marker='.', label='sgd (AUROC = %0.3f)' % sgd_auc)
plt.plot(sclf_fpr, sclf_tpr, marker='.', label='sclf (AUROC = %0.3f)' % sclf_auc)

# Title
plt.title('ROC Plot')
# Axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# Show legend
plt.legend() # 
# Show plot
plt.show()